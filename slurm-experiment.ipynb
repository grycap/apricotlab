{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f60f9c8",
   "metadata": {},
   "source": [
    "# üß™ Experiment: Deploying and Testing a SLURM Cluster for Distributed Computing\n",
    "*This tutorial will guide you through deploying a small SLURM cluster using APRICOT and the Infrastructure Manager (IM).*\n",
    "\n",
    "üìò **Context & Objective**\n",
    "\n",
    "High-Performance Computing (HPC) clusters are essential for computationally intensive tasks such as simulations, modeling, and large-scale data processing. The **SLURM (Simple Linux Utility for Resource Management)** workload manager is widely used in academic and research environments to manage and schedule computing jobs on clusters.\n",
    "\n",
    "This experiment demonstrates how to use the **APRICOT** extension to:\n",
    "\n",
    "- Deploy a SLURM cluster on a cloud provider using a predefined recipe\n",
    "\n",
    "- Submit and monitor a job directly from the notebook\n",
    "\n",
    "- Retrieve the job output\n",
    "\n",
    "- Tear down the infrastructure once the work is complete\n",
    "\n",
    "All steps are automated using `%apricot` magic commands for simplicity and reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d2fc3e",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è **Step 1: Load the APRICOT Extension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c2c39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext apricot_magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b4cb9",
   "metadata": {},
   "source": [
    "### üîë **Step 2: Add Your EGI Refresh Token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46050485",
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_token = \"<token>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71cc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot_token {refresh_token}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33597ab",
   "metadata": {},
   "source": [
    "### üìú **Step 3: Define the SLURM Cluster Recipe**\n",
    "\n",
    "You can either:\n",
    "\n",
    "- Use a **predefined SLURM recipe** via the APRICOT GUI menu, or\n",
    "\n",
    "- Use the **custom recipe** below in **TOSCA** format:\n",
    "> üîç Change both _image_ values with the valid cloud provider image you want to use.\n",
    "\n",
    "> ‚úçÔ∏è You will need to fill your authfile in `resources/authfile` with your IM and cloud credentials if you use the *magic commands* to deploy the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e27fc21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "slurm_cluster_recipe = \"\"\"\n",
    "tosca_definitions_version: tosca_simple_yaml_1_0\n",
    "\n",
    "description: Minimal SLURM Virtual Cluster\n",
    "\n",
    "imports:\n",
    "  - grycap_custom_types: https://raw.githubusercontent.com/grycap/tosca/main/custom_types.yaml\n",
    "\n",
    "topology_template:\n",
    "  inputs:\n",
    "    fe_cpus:\n",
    "      type: integer\n",
    "      default: 1\n",
    "    fe_mem:\n",
    "      type: scalar-unit.size\n",
    "      default: 1 GiB\n",
    "    wn_cpus:\n",
    "      type: integer\n",
    "      default: 1\n",
    "    wn_mem:\n",
    "      type: scalar-unit.size\n",
    "      default: 1 GiB\n",
    "    wn_num:\n",
    "      type: integer\n",
    "      default: 1\n",
    "    slurm_version:\n",
    "      type: string\n",
    "      default: 23.11.8\n",
    "    fe_ports:\n",
    "      type: map\n",
    "      default:\n",
    "        port_22:\n",
    "          protocol: tcp\n",
    "          source: 22\n",
    "\n",
    "  node_templates:\n",
    "    lrms_server:\n",
    "      type: tosca.nodes.indigo.Compute\n",
    "      properties:\n",
    "        instance_name: slurm_frontend\n",
    "      capabilities:\n",
    "        host:\n",
    "          properties:\n",
    "            num_cpus: { get_input: fe_cpus }\n",
    "            mem_size: { get_input: fe_mem }\n",
    "        os:\n",
    "          properties:\n",
    "            type: linux\n",
    "            distribution: ubuntu\n",
    "            image: one://osenserver/image-id\n",
    "        endpoint:\n",
    "          properties:\n",
    "            network_name: PUBLIC\n",
    "            ports: { get_input: fe_ports }\n",
    "            dns_name: slurmserver\n",
    "\n",
    "    lrms_front_end:\n",
    "      type: tosca.nodes.indigo.LRMS.FrontEnd.Slurm\n",
    "      properties:\n",
    "        version: { get_input: slurm_version }\n",
    "        wn_ips: { get_attribute: [lrms_wn, private_address] }\n",
    "      requirements:\n",
    "        - host: lrms_server\n",
    "\n",
    "    lrms_wn:\n",
    "      type: tosca.nodes.indigo.Compute\n",
    "      properties:\n",
    "        instance_name: slurm_worker\n",
    "      capabilities:\n",
    "        host:\n",
    "          properties:\n",
    "            num_cpus: { get_input: wn_cpus }\n",
    "            mem_size: { get_input: wn_mem }\n",
    "        os:\n",
    "          properties:\n",
    "            type: linux\n",
    "            distribution: ubuntu\n",
    "            image: one://osenserver/image-id\n",
    "        scalable:\n",
    "          properties:\n",
    "            count: { get_input: wn_num }\n",
    "\n",
    "    wn_node:\n",
    "      type: tosca.nodes.indigo.LRMS.WorkerNode.Slurm\n",
    "      properties:\n",
    "        version: { get_input: slurm_version }\n",
    "        front_end_ip: { get_attribute: [lrms_server, private_address, 0] }\n",
    "        public_front_end_ip: { get_attribute: [lrms_server, public_address, 0] }\n",
    "      requirements:\n",
    "        - host: lrms_wn\n",
    "\n",
    "  outputs:\n",
    "    cluster_ip:\n",
    "      value: { get_attribute: [lrms_server, public_address, 0] }\n",
    "    cluster_creds:\n",
    "      value: { get_attribute: [lrms_server, endpoint, credential, 0] }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0c7c9",
   "metadata": {},
   "source": [
    "### üöÄ **Step 4: Deploy the SLURM Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3f630",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%apricot_create {slurm_cluster_recipe}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c8be72",
   "metadata": {},
   "source": [
    "üìù After running this command, copy the `infrastructure_id` from the output.\n",
    "Let‚Äôs assign it to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234056dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "infrastructure_id = \"infra-id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf0841",
   "metadata": {},
   "source": [
    "### üìã **Step 5: View cluster state**\n",
    "\n",
    "You can check the logs of the deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b902b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%apricot_log {infrastructure_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295052d",
   "metadata": {},
   "source": [
    "### üß™ **Step 6: Submit a SLURM Job**\n",
    "\n",
    "Let‚Äôs submit a simple SLURM job to verify that everything is working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbade2",
   "metadata": {},
   "source": [
    "#### üìÑ **6.1 Create a SLURM job script on the VM**\n",
    "We'll create a basic SLURM job script that prints a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f91ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "script = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=test\n",
    "#SBATCH --output=output.out\n",
    "\n",
    "echo \"Hello from SLURM!\"\n",
    "\"\"\"\n",
    "\n",
    "# Write the script to /home/slurm/job.sh as the slurm user\n",
    "%apricot exec {infrastructure_id} echo {script!r} | sudo -u slurm tee /home/slurm/job.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8489e56",
   "metadata": {},
   "source": [
    "#### üì§ **6.2 Submit the job**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274444a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Submit the script using sbatch as the slurm user\n",
    "%apricot exec {infrastructure_id} sudo su - slurm -c 'sbatch /home/slurm/job.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cffb8",
   "metadata": {},
   "source": [
    "#### üìã **6.3 Check the job queue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a3618",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# View the current SLURM job queue\n",
    "%apricot exec {infrastructure_id} squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f8921",
   "metadata": {},
   "source": [
    "> Wait until your job finishes. It should be quick for this simple example.\n",
    "\n",
    "Check that the output file has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fffb09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot exec {infrastructure_id} sudo -u slurm ls /home/slurm/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feab8091",
   "metadata": {},
   "source": [
    "### üìÇ **Step 7: Retrieve the Output**\n",
    "\n",
    "After the job completes, the output will be written to a file called `output.out` in the SLURM user‚Äôs home directory. Move it to /tmp so it‚Äôs accessible for download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%apricot exec {infrastructure_id} sudo -u slurm mv /home/slurm/output.out /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72df278",
   "metadata": {},
   "source": [
    "### üì§ **Step 8: Download Output Logs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f55d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%apricot_download {infrastructure_id} /tmp/output.out ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a2121",
   "metadata": {},
   "source": [
    "### üßπ **Step 9: Clean Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bfcfbc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%apricot_destroy {infrastructure_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec313f35",
   "metadata": {},
   "source": [
    "‚úÖ **Summary**\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "- Deployed a SLURM cluster from Jupyter\n",
    "\n",
    "- Created and submitted a SLURM job\n",
    "\n",
    "- Retrieved the output and displayed it\n",
    "\n",
    "üí° **Notes**\n",
    "\n",
    "- The SLURM controller and compute nodes are automatically configured via the recipe.\n",
    "\n",
    "- SLURM jobs must be submitted as the slurm user.\n",
    "\n",
    "- Output files written in the SLURM user‚Äôs home directory aren't accessible by default‚Äîuse /tmp to enable downloads.\n",
    "\n",
    "\n",
    "üìå **Conclusion**\n",
    "\n",
    "This experiment showcases the power of cloud-based virtualization for enabling accessible HPC workflows. Using **APRICOT**, researchers and students can deploy scalable, reproducible environments directly from notebooks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
